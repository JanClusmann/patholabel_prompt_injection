---
title: "Patho Label Assessment"
output: html_document
date: "2024-10-21"
---

# Libraries
```{r}
library(ggplot2)
library(readxl)
library(dplyr)
library(tidyr)
library(gridExtra)
library(FSA)
library(rstatix)
library(scales)
library(RColorBrewer)
library(openxlsx)
options(scipen = 999)
library(svglite)
library(networkD3)
library(dplyr)
library(htmlwidgets)
library(webshot2)
library(rsvg)
Sys.setenv(CHROMOTE_CHROME="C:/Users/janni/AppData/Local/Google/Chrome/Application/chrome.exe")
```

```{r}
# installed_packages <- installed.packages()
# package_list <- c("ggplot2", "readxl", "dplyr", "tidyr", "gridExtra", "FSA", "rstatix", "scales", "RColorBrewer", "openxlsx", "svglite")
# 
# for (package in package_list) {
#   version <- installed_packages[package, "Version"]
#   if (!is.na(version)) {
#     cat(sprintf("%s==%s\n", package, version))
#   } else {
#     cat(sprintf("%s not found\n", package))
#   }
# }
```


# Loading data
```{r}
data <- read_excel("C:/Users/janni/OneDrive/Dokumente/GitHub/patholabel_prompt_injection/combined_analysis_results.xlsx")



data <- data %>%
  rowwise() %>%
  mutate(
    Label_Type = case_when(
      Label_Type == "false" ~ "Misleading \nLabel",
      Label_Type == "none" ~ "No Label",
      Label_Type == "true" ~ "True Label",
      TRUE ~ Label_Type), # keeps any other values unchanged)
    Project_Part = case_when(
      Project_Part == "LKN" ~ "Lymph node infiltration",
      Project_Part == "Molecular_Status" ~ "Mutational Status",
      TRUE ~ Project_Part
    )
  )



data_long <- data %>%
  pivot_longer(
    cols = c(starts_with("score_"), starts_with("diag_"), starts_with("flag_"), starts_with("distance_")),
    names_to = c(".value", "set"),
    names_pattern = "(.+)_(\\d+)$"
  ) %>%
  mutate(
    score = as.numeric(score),
    distance = as.numeric(distance),
    flag = as.numeric(flag),
    set = as.numeric(set)
  ) %>%
  arrange(Patient_ID_File_Name, model_name, Project_Part, Label_Type, set)

data <- data %>%
  rowwise() %>%
  mutate(
    score_mean = mean(c(score_1, score_2, score_3), na.rm = TRUE),
    distance_mean = mean(c(distance_1, distance_2, distance_3), na.rm = TRUE),
    flag_mean = mean(c(flag_1, flag_2, flag_3), na.rm = TRUE)
  ) %>%
  ungroup()






data_wm <- read_excel("C:/Users/janni/OneDrive/Dokumente/GitHub/patholabel_prompt_injection/combined_analysis_results_watermark.xlsx")

data_wm <- data_wm %>%
  rowwise() %>%
  mutate(
    Label_Type = case_when(
      Label_Type == "false" ~ "Misleading \nWatermark",
      Label_Type == "none" ~ "No Watermark",
      Label_Type == "true" ~ "Watermark leaks \nground truth",
      TRUE ~ Label_Type)) # keeps any other values unchanged)

data_long_wm <- data_wm %>%
  pivot_longer(
    cols = c(starts_with("score_"), starts_with("diag_")),
    names_to = c(".value", "set"),
    names_pattern = "(.+)_(\\d+)$"
  ) %>%
  mutate(
    score = as.numeric(score),
    set = as.numeric(set)
  ) %>%
  arrange(Patient_ID_File_Name, model_name, Project_Part, Label_Type, set)


data_wm <- data_wm %>%
  rowwise() %>%
  mutate(
    score_mean = mean(c(score_1, score_2, score_3), na.rm = TRUE)
  ) %>%
  ungroup()


data_eng <- read_excel("C:/Users/janni/OneDrive/Dokumente/PostDoc/Projects/Patho Prompt Injection/Data/combined_analysis_results_engineered.xlsx")



data_long_eng <- data_eng %>%
  pivot_longer(
    cols = c(starts_with("score_"), starts_with("diag_")),
    names_to = c(".value", "set"),
    names_pattern = "(.+)_(\\d+)$"
  ) %>%
  mutate(
    score = as.numeric(score),
    set = as.numeric(set)
  ) %>%
  arrange(Patient_ID_File_Name, model_name, Project_Part, Label_Type, set)


data_eng <- data_eng %>%
  rowwise() %>%
  mutate(
    score_mean = mean(c(score_1, score_2, score_3), na.rm = TRUE)
  ) %>%
  ungroup()

#Merge dataframes for figure 4 (prompt engineering vs no prompt engineering)

data_long_temp <- data_long %>%
  select(c("Project_Part", "Label_Type", "model_name", "score"))
data_long_temp$Prompt_Engineering <- 0 #Add this column as this is w/o engineering
  

data_long_wm_temp <- data_long_wm %>%
  select(c("Project_Part", "Label_Type", "model_name", "score"))
data_long_wm_temp$Prompt_Engineering <- 0 #Add this column as this is w/o engineering

data_long_eng_temp <- data_long_eng %>%
  select(c("Project_Part", "Label_Type", "model_name", "score", "Prompt_Engineering"))



data_fig4 <- rbind(data_long_temp, data_long_eng_temp, data_long_wm_temp)


data_fig4 <- data_fig4 %>%
  rowwise() %>%
  mutate(
    Label_Type = case_when(
      Label_Type == "Misleading \nWatermark" ~ "Misleading Label",
      Label_Type == "Misleading \nLabel" ~ "Misleading Label",
      Label_Type == "false" ~ "Misleading Label",
      Label_Type == "none" ~ "No Label",
      Label_Type == "No Watermark" ~ "No Label",
      Label_Type == "true" ~ "True Label",
      Label_Type == "Watermark leaks \nground truth" ~ "True Label",
      TRUE ~ Label_Type),
    Prompt_Engineering = case_when(
      Prompt_Engineering == 0 ~ "Native Prompt",
      Prompt_Engineering == 1 ~ "Attention on tissue"
    )) # keeps any other values unchanged)


# Order prompt engineering via factor
pe_order <- c("Native Prompt", "Attention on tissue")
data_fig4$Prompt_Engineering <- factor(data_fig4$Prompt_Engineering, levels = pe_order)




# Define variables for figures and tables
models_to_include_1 <- c("GPT-4o")
models_to_include_2 <- c("GPT-4o", "Claude-3", "Claude-3.5", "Reka Core")

label_size <- 20
fig_path <- "C:/Users/janni/OneDrive/Dokumente/PostDoc/Projects/Patho Prompt Injection/Figures/"
suppl_path <- "C:/Users/janni/OneDrive/Dokumente/PostDoc/Projects/Patho Prompt Injection/Supplementary_Material_1.xlsx"

```

#Define Export summary statistics/Color Scheme
```{r}
export_stats_to_excel <- function(excel_file, sheet_name, summary_stats, p_values, project_part, overwrite_sheet = TRUE) {
  wb <- loadWorkbook(excel_file)
  # Check if the sheet exists and handle according to overwrite_sheet parameter
  if (sheet_name %in% names(wb)) {
    if (overwrite_sheet) {
      removeWorksheet(wb, sheet_name)
      addWorksheet(wb, sheet_name)
    } else {
      stop(paste("Sheet", sheet_name, "already exists and overwrite_sheet is set to FALSE"))
    }
  } else {
    addWorksheet(wb, sheet_name)
  }
  
  # Write summary statistics
  writeData(wb, sheet_name, x = paste("Summary statistics for Project Part:", project_part), startRow = 1, startCol = 1)
  writeData(wb, sheet_name, summary_stats, startRow = 2, startCol = 1)
  
  # Write p-values
  start_row <- nrow(summary_stats) + 4
  for (i in seq_along(p_values)) {
    writeData(wb, sheet_name, names(p_values)[i], startRow = start_row, startCol = 1)
    p_value_data <- p_values[[i]]
    if (is.data.frame(p_value_data)) {
      writeDataTable(wb, sheet_name, p_value_data, startRow = start_row + 1, startCol = 1)
      start_row <- start_row + nrow(p_value_data) + 3
    } else {
      writeData(wb, sheet_name, capture.output(print(p_value_data)), startRow = start_row + 1, startCol = 1)
      start_row <- start_row + length(capture.output(print(p_value_data))) + 2
    }
  }
  
  # Save the workbook
  saveWorkbook(wb, excel_file, overwrite = TRUE)
  
  print(paste("Data has been exported to", sheet_name, "in the Excel file."))
}
```

#Define theme for visuals
```{r}
custom_theme <- theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = 10, colour = "black", angle = 45, hjust = 1, vjust = 1),
    axis.text.y = element_text(size = 10, colour = "black"),
    axis.title.y = element_text(size = 10, vjust = 2),
    legend.title = element_blank(),
    legend.text = element_text(size = 10),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
    plot.margin = margin(1, 1, 2, 0.5, "cm")
  )

custom_colors_1 <- c("#22628F", "#20B38E", "#CC9439", "#BD672A")
custom_colors <- c("#999999", "#20B38E", "#CC9439", "#BD672A")
custom_colors_2 <- c("#20B38E", "#CC9439", "#BD672A")

```


# Figure 2 Accuracy all tasks combined

```{r}
# Calculate summary statistics for Figure 2 (b)
summary_stats_2a <- data %>%
  group_by(model_name, Label_Type) %>%
  summarize(
    mean = mean(score_mean, na.rm = TRUE),
    sd = sd(score_mean, na.rm = TRUE),
    .groups = 'drop'
  )

# Merge summary statistics back to the original data for plotting
data_2a <- data %>%
  left_join(summary_stats_2a, by = c("model_name", "Label_Type")) 


dot_counts <- data_2a %>%
  group_by(model_name) %>%
  summarize(dot_count = n())

# Print the number of dots per Model
print("Number of dots per Model:")
print(dot_counts)

label_size <- 20


custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138")

# Create the plot with error bars
plot_2a <- ggplot(data_2a, aes(x = Label_Type, y = mean, fill = `model_name`)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), alpha = 1) +
  geom_jitter(aes(y = score_mean, color = `Label_Type`), position = position_jitterdodge(dodge.width = 0.9, jitter.width = 0.15, jitter.height = 0.02), shape = 21, fill = NA, size = 1.5, stroke = 0.3) +
  geom_errorbar(aes(ymin = pmax(mean - sd, 0), ymax = pmin(mean + sd, 1)), width = 0, size = 0.5, color = "black", position = position_dodge(width = 0.9)) +
  labs(y = "Accuracy") +
  scale_fill_manual(values = custom_colors) + 
  scale_color_manual(values = rep("black", 3)) +
  #scale_fill_manual(values = c("True Label" = "#999999", "Misleading \nLabel" = "#fbc9c4", "No Label" = "blue")) +
  #scale_color_manual(values = c("True Label" = "black", "Misleading \nLabel" = "black", "No Label" = "black")) +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = label_size, colour = "black", vjust = 1, hjust = 1, angle = 45),
    axis.text.y = element_text(size = label_size, colour = "black"),
    axis.title.y = element_text(size = label_size, vjust = 2),
    legend.title = element_blank(),
    
    legend.text = element_text(size = label_size),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
    plot.margin = margin(0.5, 0.5, 1, 0.5, "cm")
  ) +
  scale_y_continuous(limits = c(-0.05, 1.05), expand = c(0, 0))
  coord_fixed(ratio = 5 / 1)

# Save the plot
ggsave(filename = paste0(fig_path, "Accuracy_all_tasks.svg"), plot = plot_2a, width = 6, height = 6, bg = "transparent")

```







```{r}
create_project_plot <- function(data, 
                              metric = "score_mean",  # Options: "score_mean", "distance_mean", "flag_mean"
                              y_label = "Accuracy",
                              custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138"),
                              fig_path = "figures/",
                              label_size = 12) {
  
  # Define custom theme if not already defined
  custom_theme <- theme_minimal() +
    theme(
      text = element_text(size = label_size),
      plot.title = element_text(size = label_size + 2, face = "bold"),
      axis.text = element_text(size = label_size, color = "black"),
      axis.title = element_text(size = label_size),
      legend.text = element_text(size = label_size),
      panel.grid.minor = element_blank()
    )
  
  # Calculate summary statistics for each project part
  summary_stats <- data %>%
    group_by(model_name, Label_Type, Project_Part) %>%
    summarize(
      mean = mean(!!sym(metric), na.rm = TRUE),
      sd = sd(!!sym(metric), na.rm = TRUE),
      .groups = 'drop'
    )
  
  # Get unique project parts
  project_parts <- unique(data$Project_Part)
  
  # Create a plot for each project part
  plots <- lapply(project_parts, function(part) {
    # Filter data for current project part
    data_subset <- data %>% filter(Project_Part == part)
    stats_subset <- summary_stats %>% filter(Project_Part == part)
    
    # Create plot
    p <- ggplot(stats_subset, aes(x = Label_Type, y = mean, fill = model_name)) +
      geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
      geom_errorbar(aes(ymin = pmax(mean - sd, 0), ymax = pmin(mean + sd, 1)), 
                    width = 0, size = 0.5, color = "black", 
                    position = position_dodge(width = 0.8)) +
      geom_jitter(data = data_subset, aes(y = !!sym(metric), group = model_name), 
                  position = position_jitterdodge(dodge.width = 0.8, 
                                                jitter.width = 0.15, 
                                                jitter.height = 0.025),
                  size = 1, alpha = 0.4, stroke = 0.3) +
      labs(y = y_label,
           title = part) +
      theme_minimal() +
      custom_theme + 
      theme(
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = label_size, colour = "black", 
                                 angle = 45, vjust = 1, hjust = 1),
        axis.text.y = element_text(size = label_size, colour = "black"),
        axis.title.y = element_text(size = label_size, vjust = 2),
        legend.title = element_blank(),
        legend.text = element_text(size = label_size),
        legend.position = "top",
        legend.direction = "horizontal",
        legend.justification = "center",
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
        plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
      ) +
      scale_fill_manual(values = custom_colors) +
      scale_y_continuous(limits = c(-0.05, 1.05), expand = c(0, 0)) +
      scale_x_discrete(expand = expansion(add = 0.7)) +
      coord_fixed(ratio = 3.2 / 1)
    
    # Save the plot
    metric_name <- gsub("_mean", "", metric)  # Remove '_mean' from metric name
    filename <- paste0(fig_path, 
                      metric_name, "_", 
                      gsub("-", "_", part),  # Replace hyphens with underscores
                      "_plot.svg")
    ggsave(filename = filename, plot = p, width = 12, height = 6, 
           bg = "transparent")
    
    return(p)
  })
  
  # Name the plots list with project parts
  names(plots) <- project_parts
  
  return(plots)
}
```

```{r}
plots <- create_project_plot(
  data = data,
  metric = "score_mean",  # or "distance_mean" or "flag_mean"
  y_label = "Accuracy",
  custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138"),
  fig_path = fig_path,
  label_size = 18
)
```


```{r}
plots <- create_project_plot(
  data = data,
  metric = "flag_mean",  # or "distance_mean" or "flag_mean"
  y_label = "Flagged as suspicious/incoherent",
  custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138"),
  fig_path = fig_path,
  label_size = 18
)
```


```{r}
plots <- create_project_plot(
  data = data,
  metric = "distance_mean",  # or "distance_mean" or "flag_mean"
  y_label = "Distance from truth",
  custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138"),
  fig_path = fig_path,
  label_size = 18
)
```


# Figure 3: Watermarks
```{r}
fig_path

plots <- create_project_plot(
  data = data_wm,
  metric = "score_mean",  # or "distance_mean" or "flag_mean"
  y_label = "Accuracy",
  custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138"),
  fig_path = paste0(fig_path, "Watermark"),
  label_size = 16
)
```



```{r}
data_prostate <- data_wm[data_wm$True_Prompt=="Prostate", ]

plots <- create_project_plot(
  data = data_prostate,
  metric = "score_mean",  # or "distance_mean" or "flag_mean"
  y_label = "Accuracy",
  custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138"),
  fig_path = paste0(fig_path, "Watermark_Prostate"),
  label_size = 16
)
```

```{r}
data_ovary <- data_wm[data_wm$True_Prompt=="Ovary", ]

plots <- create_project_plot(
  data = data_ovary,
  metric = "score_mean",  # or "distance_mean" or "flag_mean"
  y_label = "Accuracy",
  custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138"),
  fig_path = paste0(fig_path, "Watermark_Ovary"),
  label_size = 16
)
```



# FIgure 3 Sankey Plot Function

```{r}

data_sankey <- data_long_wm %>%
  mutate(Label_Type = gsub("\n", " ", Label_Type))


create_sankey <- function(df, fig_path, width, height, organ = NULL, model = NULL) {
  # Create title based on parameters
  title <- if(!is.null(model)) {
    paste0(organ, " - ", model)
  } else {
    paste0(organ, " - All Models")
  }

  # Create the long format data for Sankey diagram
  data_long <- df %>%
    filter(!is.na(diag), !is.na(set)) %>%
    group_by(Label_Type, diag) %>%
    summarise(value = n(), .groups = 'drop')

  # Create nodes dataframe
  nodes <- data.frame(
    name = c(as.character(unique(data_long$Label_Type)), 
             as.character(unique(data_long$diag))) %>% 
    unique()
  )

  # Match IDs
  data_long$IDsource <- match(data_long$Label_Type, nodes$name) - 1
  data_long$IDtarget <- match(data_long$diag, nodes$name) - 1

  # Create color scale
  # ColourScal <- 'd3.scaleOrdinal() .range(["#FDE725FF","#B4DE2CFF","#6DCD59FF","#35B779FF","#1F9E89FF","#26828EFF","#31688EFF","#3E4A89FF"])'
  ColourScal <- 'd3.scaleOrdinal()
    .domain(["Misleading Watermark", "No Watermark", "Watermark leaks ground truth", 
             "Prostate", "Colorectal", "Ovary", "Breast", "Lung"])
    .range(["#BD5138", "#999999", "#20B38E", 
            "#7E1918", "#00AEEF", "#D97D25", "#ED2891", "#542C88"])'

  # Create Sankey diagram
  p <- sankeyNetwork(Links = data_long, 
                    Nodes = nodes,
                    Source = "IDsource", 
                    Target = "IDtarget",
                    Value = "value", 
                    NodeID = "name",
                    sinksRight=TRUE, 
                    colourScale=ColourScal, 
                    nodeWidth=13, 
                    fontSize=12, 
                    fontFamily="Arial",
                    nodePadding=6,
                    width=width,
                    height=height)

   # Create filenames for both HTML and SVG
  # Create filenames
  html_filename <- paste0(fig_path, organ, "", if(!is.null(model)) paste0(gsub("-", "", model), ""), "sankey.html")
  saveWidget(p, html_filename, selfcontained = TRUE)

  pdf_filename <- gsub(".html", ".pdf", html_filename)
  webshot2::webshot(
    url = html_filename,
    file = pdf_filename,
    vwidth = width,
    vheight = height,
    zoom = 1,
    selector= "body",
  )
  return(pdf_filename)
  }
# Example usage: 
## Note: HTML to PDF conversion works only when Chrome and Adobe REader are closed
df_ovary <- data_sankey %>%
  filter(Class_Ground_Truth == "Ovary")


result <- create_sankey(df_ovary, fig_path = fig_path, width=250, height=200, organ = "Ovary")

```


```{r}
# For specific model
df_ovary_claude3 <- data_long_wm %>%
  filter(Class_Ground_Truth == "Ovary",
         model_name == "Claude-3")
create_sankey(df_ovary_claude3, fig_path = fig_path, 
             organ = "Ovary", model = "Claude-3")
```

```{r}
df_prostate <- data_long_wm %>%
  filter(Class_Ground_Truth == "Prostate")
create_sankey(df_prostate, fig_path = fig_path, width=250, height=200, organ = "Prostate")

```


# Figure 4: prompt Engineering




```{r}
create_mitigation_plot <- function(data, 
                              metric = "score_mean",  # Options: "score_mean", "distance_mean", "flag_mean"
                              y_label = "Accuracy",
                              custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138"),
                              fig_path = "figures/",
                              label_size = 12,
                              name= "Mitigation",
                              legend_position= "none") {
  
  # Define custom theme if not already defined
  custom_theme <- theme_minimal() +
    theme(
      text = element_text(size = label_size),
      plot.title = element_text(size = label_size + 2, face = "bold"),
      axis.text = element_text(size = label_size, color = "black"),
      axis.title = element_text(size = label_size),
      legend.text = element_text(size = label_size),
      panel.grid.minor = element_blank()
    )
  
  # Calculate summary statistics for each 
  summary_stats <- data %>%
    group_by(model_name, Label_Type, Prompt_Engineering) %>%
    summarize(
      mean = mean(!!sym(metric), na.rm = TRUE),
      sd = sd(!!sym(metric), na.rm = TRUE),
      .groups = 'drop'
    )
  
  # Get unique project parts
  project_parts <- unique(data$Label_Type)
  print(project_parts)
  
  # Create a plot for each project part
  plots <- lapply(project_parts, function(part) {
    # Filter data for current project part
    data_subset <- data %>% filter(Label_Type == part)
    stats_subset <- summary_stats %>% filter(Label_Type == part)
    
    # Create plot
    p <- ggplot(stats_subset, aes(x = Prompt_Engineering, y = mean, fill = model_name)) +
      geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
      geom_errorbar(aes(ymin = pmax(mean - sd, 0), ymax = pmin(mean + sd, 1)), 
                    width = 0, size = 0.5, color = "black", 
                    position = position_dodge(width = 0.8)) +
      geom_jitter(data = data_subset, aes(y = !!sym(metric), group = model_name), 
                  position = position_jitterdodge(dodge.width = 0.8, 
                                                jitter.width = 0.15, 
                                                jitter.height = 0.025),
                  size = 1, alpha = 0.4, stroke = 0.3) +
      labs(y = y_label,
           title = part) +
      theme_minimal() +
      custom_theme + 
      theme(
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = label_size, colour = "black", 
                                 angle = 45, vjust = 1, hjust = 1),
        axis.text.y = element_text(size = label_size, colour = "black"),
        axis.title.y = element_text(size = label_size, vjust = 2),
        legend.title = element_blank(),
        legend.text = element_text(size = label_size),
        legend.position = legend_position,
        legend.direction = "horizontal",
        legend.justification = "center",
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
        plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
      ) +
      scale_fill_manual(values = custom_colors) +
      scale_y_continuous(limits = c(-0.05, 1.05), expand = c(0, 0)) +
      scale_x_discrete(expand = expansion(add = 0.7)) +
      coord_fixed(ratio = 3.2 / 1)
    
    # Save the plot
    
    filename <- paste0(fig_path, name, "_", part, "_plot.svg")
    ggsave(filename = filename, plot = p, width = 12, height = 6, 
           bg = "transparent")
    
    return(p)
  })
  
  # Name the plots list with project parts
  names(plots) <- project_parts
  
  return(plots)
}
```

```{r}

plots <- create_mitigation_plot(
  data = data_fig4,
  metric = "score",  # or "distance_mean" or "flag_mean"
  y_label = "Accuracy",
  custom_colors = c("#22628F", "#20B38E", "#CC9439", "#BD5138"),
  fig_path = paste0(fig_path, "prompt_engineering"),
  label_size = 16
)
```
# Stats Fig 2a:

```{r}
summary_figure_2a <- data %>%
  group_by(Label_Type, model_name) %>%
  summarize(
    mean_accuracy=mean(score_mean, na.rm = TRUE),
    sd_accuracy  =  sd(score_mean, na.rm = TRUE)
  )

print(summary_figure_2a)


figure_2a_test <- kruskal.test(score_mean ~ Label_Type, data = data )
figure_2a_test$p.value <- round(figure_2a_test$p.value, 5)
print(figure_2a_test)

# Post-hoc analysis using Dunn test if Kruskal-Wallis test is significant

dunn_test <- dunnTest(score_mean ~ Label_Type, data = data, method = "bonferroni")
dunn_test$res$P.adj <- round(dunn_test$res$P.adj, 5)
print(dunn_test)


# Create a named list of p-values
p_values_list <- list(
  "Kruskal-Wallis Test" = figure_2a_test,
  "Dunn Test" = dunn_test$res
)

# Use the function
export_stats_to_excel(
  excel_file = suppl_path,
  sheet_name = "ST4 Statistics Figure 2a",
  summary_stats = summary_figure_2a,
  p_values = p_values_list
)
```

# Stats Fig 2c-e:

```{r}

# Dictionary for project parts and figure legends
figure_legend_map <- list(
  "T-Stage" = "2c",
  "Lymph node infiltration" = "2d",
  "Mutational Status" = "2e"
)

# List of unique project parts
project_parts <- names(figure_legend_map)

# Loop through each project part
for (i in seq_along(project_parts)) {
  project_part <- project_parts[i]
  figure_legend <- figure_legend_map[[project_part]] # Get the corresponding figure legend
  
  # Filter data for the current project part
  subset_data <- data %>% filter(Project_Part == project_part)
  
  # Summary statistics
  summary_figure_2a <- subset_data %>%
    group_by(Label_Type, model_name) %>%
    summarize(
      mean_accuracy = mean(score_mean, na.rm = TRUE),
      sd_accuracy   = sd(score_mean, na.rm = TRUE)
    )
  
  print(summary_figure_2a)
  
  # Kruskal-Wallis test
  figure_2a_test <- kruskal.test(score_mean ~ Label_Type, data = subset_data)
  figure_2a_test$p.value <- round(figure_2a_test$p.value, 5)
  print(figure_2a_test)
  
  # Post-hoc Dunn test
  dunn_test <- dunnTest(score_mean ~ Label_Type, data = subset_data, method = "bonferroni")
  dunn_test$res$P.adj <- round(dunn_test$res$P.adj, 5)

  print(dunn_test)
  
  # Create a named list of p-values
  p_values_list <- list(
    "Kruskal-Wallis Test" = figure_2a_test,
    "Dunn Test" = dunn_test$res
  )
  
  # Export results to an Excel file with distinct sheet names
  export_stats_to_excel(
    excel_file = suppl_path,
    sheet_name = paste0("ST ", 4 + i, " Figure " ,figure_legend, " Score"), # Dynamically name sheets using figure legend
    summary_stats = summary_figure_2a,
    p_values = p_values_list,
    project_part = project_part
  )
}


```
# Stats Fig 2f-h:

```{r}
# Dictionary for project parts and figure legends
figure_legend_map <- list(
  "T-Stage" = "2c",
  "Lymph node infiltration" = "2d",
  "Mutational Status" = "2e"
)

# List of unique project parts
project_parts <- names(figure_legend_map)

# Loop through each project part
for (i in seq_along(project_parts)) {
  project_part <- project_parts[i]
  figure_legend <- figure_legend_map[[project_part]] # Get the corresponding figure legend
  
  # Filter data for the current project part
  subset_data <- data %>% filter(Project_Part == project_part)
  
  # Summary statistics
  summary_figure_2a <- subset_data %>%
    group_by(Label_Type, model_name) %>%
    summarize(
      mean_accuracy = mean(flag_mean, na.rm = TRUE),
      sd_accuracy   = sd(flag_mean, na.rm = TRUE)
    )
  
  print(summary_figure_2a)
  
  # Kruskal-Wallis test
  figure_2a_test <- kruskal.test(flag_mean ~ Label_Type, data = subset_data)
  figure_2a_test$p.value <- round(figure_2a_test$p.value, 5)
  print(figure_2a_test)
  
  # Post-hoc Dunn test
  dunn_test <- dunnTest(flag_mean ~ Label_Type, data = subset_data, method = "bonferroni")
  dunn_test$res$P.adj <- round(dunn_test$res$P.adj, 5)

  print(dunn_test)
  
  # Create a named list of p-values
  p_values_list <- list(
    "Kruskal-Wallis Test" = figure_2a_test,
    "Dunn Test" = dunn_test$res
  )
  
  # Export results to an Excel file with distinct sheet names
  export_stats_to_excel(
    excel_file = suppl_path,
    sheet_name = paste0("ST ", 7 + i, " Figure " ,figure_legend, " Flag "), # Dynamically name sheets using figure legend
    summary_stats = summary_figure_2a,
    p_values = p_values_list,
    project_part = project_part
  )
}
```



# Stats Fig 3a, c, e:

```{r}
# Define project part map for Figure 3
figure_legend_map_3 <- list(
  "All" = "3a",
  "Ovary" = "3c",
  "Prostate" = "3e"
)

# Define filters for each table
filters <- list(
  "All" = NULL,                     # No filter for the first table
  "Ovary" = "Ovary",                # Filter for Ovary in True_Prompt
  "Prostate" = "Prostate"           # Filter for Prostate in True_Prompt
)

# Loop through each project part for Figure 3
for (i in seq_along(filters)) {
  filter_name <- names(filters)[i]
  filter_value <- filters[[filter_name]]
  figure_legend <- figure_legend_map_3[[filter_name]]
  
  # Filter data for the current subset
  subset_data <- if (is.null(filter_value)) {
    data_wm # No filter for the "All" case
  } else {
    data_wm %>% filter(True_Prompt == filter_value)
  }
  
  # Project part to be included in the header
  project_part <- paste("Watermark -", filter_name)
  
  # Summary statistics
  summary_figure_3 <- subset_data %>%
    group_by(Label_Type, model_name) %>%
    summarize(
      mean_accuracy = mean(score_mean, na.rm = TRUE),
      sd_accuracy   = sd(score_mean, na.rm = TRUE)
    )
  
  print(summary_figure_3)
  
  # Kruskal-Wallis test
  figure_3_test <- kruskal.test(score_mean ~ Label_Type, data = subset_data)
  figure_3_test$p.value <- round(figure_3_test$p.value, 5)
  print(figure_3_test)
  
  # Post-hoc Dunn test
  dunn_test <- dunnTest(score_mean ~ Label_Type, data = subset_data, method = "bonferroni")
  dunn_test$res$P.adj <- round(dunn_test$res$P.adj, 5)
  
  print(dunn_test)
  
  # Create a named list of p-values
  p_values_list <- list(
    "Kruskal-Wallis Test" = figure_3_test,
    "Dunn Test" = dunn_test$res
  )
  
  # Export results to an Excel file
  export_stats_to_excel(
    excel_file = suppl_path,
    sheet_name = paste0("ST ", 10 + i, "Fig ", figure_legend, " Stats"), # Dynamically name sheets using figure legend
    summary_stats = summary_figure_3,
    p_values = p_values_list,
    project_part = project_part  # Include project part in the header
  )
}



```
# Stats Fig 4

```{r}

# Dictionary for project parts and figure legends
figure_legend_map <- list(
  "Misleading Label" = "4a",
  "No Label" = "4b",
  "True Label" = "4c"
)

# List of unique project parts
project_parts <- names(figure_legend_map)

# Loop through each project part
for (i in seq_along(project_parts)) {
  project_part <- project_parts[i]
  figure_legend <- figure_legend_map[[project_part]] # Get the corresponding figure legend
  
  # Filter data for the current project part
  subset_data <- data_fig4 %>% filter(Label_Type == project_part)
  
  # Summary statistics
  summary_figure_2a <- subset_data %>%
    group_by(Prompt_Engineering, model_name) %>%
    summarize(
      mean_accuracy = mean(score, na.rm = TRUE),
      sd_accuracy   = sd(score, na.rm = TRUE)
    )
  
  print(summary_figure_2a)
  
  # Kruskal-Wallis test
  figure_2a_test <- kruskal.test(score ~ Prompt_Engineering, data = subset_data)
  figure_2a_test$p.value <- round(figure_2a_test$p.value, 5)
  print(figure_2a_test)
  
  # Post-hoc Dunn test
  dunn_test <- dunnTest(score ~ Prompt_Engineering, data = subset_data, method = "bonferroni")
  dunn_test$res$P.adj <- round(dunn_test$res$P.adj, 5)

  print(dunn_test)
  
  # Create a named list of p-values
  p_values_list <- list(
    "Kruskal-Wallis Test" = figure_2a_test,
    "Dunn Test" = dunn_test$res
  )
  
  project_part <- paste("Prompt_Engineering -", project_part)
  
  # Export results to an Excel file with distinct sheet names
  export_stats_to_excel(
    excel_file = suppl_path,
    sheet_name = paste0("ST ", 13 + i, " Figure " ,figure_legend, " Score"), # Dynamically name sheets using figure legend
    summary_stats = summary_figure_2a,
    p_values = p_values_list,
    project_part = project_part
  )
}


```

# _______________________________




# Figure 2: (a) Average accuracy in detecting the represented organs per model
```{r}
summary_stats_2a <- data_long %>%
  group_by(Model) %>%
  summarize(
    mean = mean(Labels, na.rm = TRUE),
    sd = sd(Labels, na.rm = TRUE),
    .groups = 'drop'
  )

# Merge summary statistics back to the original data for plotting
data_2a <- data %>%
  left_join(summary_stats_2a, by = "Model") %>%
  filter(`Adversarial prompt` == "No Prompt Injection")

dot_counts <- data_2a %>%
  group_by(Model) %>%
  summarize(dot_count = n())

# Print the number of dots per Model
print("Number of dots per Model:")
print(dot_counts)


label_size <- 25

plot_2a <- ggplot(data_2a, aes(x = Model, y = mean)) +
  geom_bar(stat = "identity", fill = "#999999", color = "#999999", alpha = 1) +
  geom_jitter(aes(y = Labels_Mean), 
              width = 0.2, height = 0.02,
              color = "black", shape = 21, size = 1.5, fill = NA, stroke = 0.5) +
  geom_errorbar(aes(ymin = pmax(mean - sd, 0), ymax = pmin(mean + sd, 1)), 
                width = 0.2, size = 0.5, color = "black") +
  labs(y = "Organ Detection Rate [%]") +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = label_size, colour = "black", vjust = 1, hjust = 1, angle = 45),
    axis.text.y = element_text(size = label_size, colour = "black"),
    axis.title.y = element_text(size = label_size, vjust = 2),
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  ) +
  scale_y_continuous(limits = c(-0.05, 1.05), expand = c(0, 0))

# Save the plot
ggsave(filename = paste0(fig_path, "Fig_2a_Organ_Detection_per_Model.svg"), plot = plot_2a, width = 4.5, height = 6, bg = "transparent", device = svglite)

```



# Figure 2 (b) Average harmfulness scores for all queries with injected prompt vs prompts without prompt injection
```{r}

# Calculate summary statistics for Figure 2 (b)
summary_stats_2b <- data_selection %>%
  group_by(Model, `Adversarial prompt`) %>%
  summarize(
    mean = mean(Harmfulness_Mean, na.rm = TRUE),
    sd = sd(Harmfulness_Mean, na.rm = TRUE),
    .groups = 'drop'
  )

# Merge summary statistics back to the original data for plotting
data_2b <- data_selection %>%
  left_join(summary_stats_2b, by = c("Model", "Adversarial prompt"))


dot_counts <- data_2b %>%
  group_by(Model) %>%
  summarize(dot_count = n())

# Print the number of dots per Model
print("Number of dots per Model:")
print(dot_counts)

label_size <- 25

# Create the plot with error bars
plot_2b <- ggplot(data_2b, aes(x = Model, y = mean, fill = `Adversarial prompt`)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), alpha = 1) +
  geom_jitter(aes(y = Harmfulness_Mean, color = `Adversarial prompt`), position = position_jitterdodge(dodge.width = 0.9, jitter.width = 0.3, jitter.height = 0.035), shape = 21, fill = NA, size = 1.5, stroke = 0.3) +
  geom_errorbar(aes(ymin = pmax(mean - sd, 0), ymax = pmin(mean + sd, 1)), width = 0, size = 0.5, color = "black", position = position_dodge(width = 0.9)) +
  labs(y = "Lesion Miss Rate [%]") +
  scale_fill_manual(values = c("No Prompt Injection" = "#999999", "Prompt Injection" = "#fbc9c4")) +
  scale_color_manual(values = c("No Prompt Injection" = "black", "Prompt Injection" = "black")) +
  theme_minimal() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = label_size, colour = "black", vjust = 1, hjust = 1, angle = 45),
    axis.text.y = element_text(size = label_size, colour = "black"),
    axis.title.y = element_text(size = label_size, vjust = 2),
    legend.title = element_blank(),
    
    legend.text = element_text(size = label_size),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
    plot.margin = margin(0.5, 0.5, 1, 0.5, "cm")
  ) +
  scale_y_continuous(limits = c(-0.05, 1.05), expand = c(0, 0))
  coord_fixed(ratio = 5 / 1)

# Save the plot
ggsave(filename = paste0(fig_path, "Harmfulness_Score_per_Model.svg"), plot = plot_2b, width = 9, height = 6, bg = "transparent")


```


# Figure 2 (c) Harmfulness scores in GPT-4 comparing position of adversarial prompt (in image itself vs in previous image vs in system prompt)
```{r}
custom_colors <- c("#22628F", "#20B38E", "#CC9439", "#BD5138")
# Set the correct order for the factor levels
position_order <- c("No Prompt Injection", "In text prompt", "In image itself", "In previous image")


# Calculate summary statistics for Figure 2 (c)
summary_stats_2c <- data_selection %>%
  group_by(Model, `Position of adversarial prompt`) %>%
  summarize(
    mean = mean(Harmfulness_Mean, na.rm = TRUE),
    sd = sd(Harmfulness_Mean, na.rm = TRUE),
    .groups = 'drop'
  )
# Merge summary statistics back to the original data for plotting
data_2c <- data_selection %>%
  left_join(summary_stats_2c, by = c("Model", "Position of adversarial prompt"))

# Convert Position of adversarial prompt to a factor with the desired order
data_2c$`Position of adversarial prompt` <- factor(data_2c$`Position of adversarial prompt`, levels = position_order)
label_size <- 20

dot_counts <- data_2c %>%
  group_by(Model) %>%
  summarize(dot_count = n())

# Print the number of dots per Model
print("Number of dots per Model:")
print(dot_counts)


# Create the plot with error bars and increased spacing
plot_2c <- ggplot(data_2c, aes(x = `Position of adversarial prompt`, y = mean, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  geom_errorbar(aes(ymin = pmax(mean - sd, 0), ymax = pmin(mean + sd, 1)), 
                width = 0, size = 0.5, color = "black", position = position_dodge(width = 0.8)) +
  geom_jitter(data = data_selection, aes(y = Harmfulness_Mean, group = Model), 
              position = position_jitterdodge(dodge.width = 0.8, jitter.width = 0.15, jitter.height = 0.025),
              size = 1, alpha = 0.4, stroke=0.3) +
  labs(y = "Lesion Miss Rate [%]") +
  theme_minimal() +
  custom_theme + 
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = label_size, colour = "black", angle = 45, vjust = 1, hjust = 1),
    axis.text.y = element_text(size = label_size, colour = "black"),
    axis.title.y = element_text(size = label_size, vjust = 2),
    legend.title = element_blank(),
    legend.text = element_text(size = label_size),
    legend.position = "top",
    legend.direction = "horizontal",
    legend.justification = "center",
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  ) +
  scale_fill_manual(values = custom_colors) +
  scale_y_continuous(limits = c(-0.05, 1.05), expand = c(0, 0)) +
  scale_x_discrete(expand = expansion(add = 0.7)) +
  coord_fixed(ratio = 3.2 / 1)

# Print and save the plot
print(plot_2c)
ggsave(filename = paste0(fig_path, "Harmfulness_Score_per_Prompt_position_2_alternativ.svg"), plot = plot_2c, width = 12, height = 6, bg = "transparent")


```


# Figure 2 (d) Harmfulness scores in GPT-4 comparing variation (high/low contrast, tiny text)


```{r}
# Figure 2 (d) Harmfulness scores comparing injection variations
custom_colors <- c("#22628F", "#20B38E", "#CC9439", "#BD5138")


# Calculate summary statistics for Figure 2 (d)
summary_stats_2d <- data_selection %>%
  group_by(Model, Variation) %>%
  summarize(
    mean = mean(Harmfulness_Mean, na.rm = TRUE),
    sd = sd(Harmfulness_Mean, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  filter(Variation != "In text prompt")

# Merge summary statistics back to the original data for plotting
data_2d <- data_selection %>%
  left_join(summary_stats_2d, by = c("Model", "Variation")) %>%
  filter(Variation != "In text prompt")

# Set the correct order for the factor levels (if needed)
variation_order <- c("No Prompt Injection", "Black on white", "Black on black", "Tiny text")
data_2d$Variation <- factor(data_2d$Variation, levels = variation_order)

label_size <- 20

# Create the plot with error bars and increased spacing
plot_2d <- ggplot(data_2d, aes(x = Variation, y = mean, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  geom_errorbar(aes(ymin = pmax(mean - sd, 0), ymax = pmin(mean + sd, 1)), 
                width = 0, size = 0.5, color = "black", position = position_dodge(width = 0.8)) +
  geom_jitter(data = data_selection %>% filter(Variation != "In text prompt"), 
              aes(y = Harmfulness_Mean, group = Model),
              position = position_jitterdodge(dodge.width = 0.8, jitter.width = 0.15, jitter.height = 0.025),
              size = 1, alpha = 0.4, stroke=0.3) +
  labs(y = "Lesion Miss Rate [%]") +
  theme_minimal() +
  custom_theme + 
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = label_size, colour = "black", angle = 45, vjust = 1, hjust = 1),
    axis.text.y = element_text(size = label_size, colour = "black"),
    axis.title.y = element_text(size = label_size, vjust = 2),
    legend.title = element_blank(),
    legend.text = element_text(size = label_size),
    legend.position = "top",
    legend.direction = "horizontal",
    legend.justification = "center",
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  ) +
  scale_fill_manual(values = custom_colors) +
  scale_y_continuous(limits = c(-0.05, 1.05), expand = c(0, 0)) +
  scale_x_discrete(expand = expansion(add = 0.7)) +
  coord_fixed(ratio = 3.2 / 1)

print(plot_2d)

# Save the plot
ggsave(filename = paste0(fig_path, "Harmfulness_Score_per_injectionvariation.svg"), plot = plot_2d, width = 12, height = 6, bg = "transparent")
```




# Suppl. Figure 3a
```{r}
custom_colors <- c("#22628F", "#20B38E", "#CC9439", "#BD5138")
label_size <- 20
plot_width <- 8
plot_height <- 6

# Figure 2 (a2)
summary_stats_2a2 <- data_selection %>%
  group_by(Model, Modality) %>%
  summarize(
    mean = mean(Labels_Mean, na.rm = TRUE),
    sd = sd(Labels_Mean, na.rm = TRUE),
    .groups = 'drop'
  )
data_2a2 <- data_selection %>%
  left_join(summary_stats_2a2, by = c("Model", "Modality"))


avg_by_modality <- summary_stats_2a2 %>%
  group_by(Modality) %>%
  summarize(avg_detection_rate = mean(mean, na.rm = TRUE))

modality_order <- avg_by_modality %>% # Order the levels of Modality factor based on the average detection rate
  arrange(desc(avg_detection_rate)) %>%
  pull(Modality)

data_2a2$Modality <- factor(data_2a2$Modality, levels = modality_order) # Update the factor levels in the main dataset


plot_2a2 <- ggplot(data_2a2, aes(x = Modality, y = mean, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  geom_errorbar(aes(ymin = pmax(mean - sd, 0), ymax = pmin(mean + sd, 1)), width = 0, size = 0.5, color = "black", position = position_dodge(width = 0.8)) +
  labs(y = "Organ Detection Rate [%]") +
  theme_minimal() + 
  custom_theme + 
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = label_size, colour = "black", angle = 45, vjust = 1, hjust=1),
    axis.text.y = element_text(size = label_size, colour = "black"),
    axis.title.y = element_text(size = label_size, vjust = 2),
    legend.title = element_blank(),
    legend.text = element_text(size = label_size),
    legend.position = "top",
    legend.justification = "center",
    legend.direction = "horizontal",
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  ) +
  scale_fill_manual(values = custom_colors) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_x_discrete(expand = expansion(add = 0.7)) +
  coord_fixed(ratio = 4 / 1)

ggsave(filename = paste0(fig_path, "organ_Score_per_Modality.svg"), plot = plot_2a2, width = plot_width, height = plot_height, bg = "transparent")



```



#Suppl. Figure 3b
```{r}
# Figure Suppl. 3 b
summary_stats_2e <- data_selection %>%
  group_by(Model, Modality) %>%
  summarize(
    mean = mean(Harmfulness_Mean, na.rm = TRUE),
    sd = sd(Harmfulness_Mean, na.rm = TRUE),
    .groups = 'drop'
  )

# Calculate overall mean per Modality
overall_mean_per_modality <- data_selection %>%
  group_by(Modality) %>%
  summarize(
    mean = mean(Harmfulness_Mean, na.rm = TRUE),
    sd = sd(Harmfulness_Mean, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  mutate(Model = "Overall")

# Combine model-specific and overall statistics
summary_stats_combined <- bind_rows(summary_stats_2e, overall_mean_per_modality)


data_2e <- data_selection %>%
  left_join(summary_stats_2e, by = c("Model", "Modality"))

data_2e$Modality <- factor(data_2e$Modality, levels = modality_order)

plot_2e <- ggplot(data_2e, aes(x = Modality, y = mean, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  geom_errorbar(aes(ymin = pmax(mean - sd, 0), ymax = pmin(mean + sd, 1)), width = 0, size = 0.5, color = "black", position = position_dodge(width = 0.8)) +
  labs(y = "Lesion Miss Rate [%]") +
  theme_minimal() + 
  custom_theme + 
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = label_size, colour = "black", angle = 45, vjust = 1, hjust=1),
    axis.text.y = element_text(size = label_size, colour = "black"),
    axis.title.y = element_text(size = label_size, vjust = 2),
    legend.title = element_blank(),
    legend.text = element_text(size = label_size),
    legend.position = "top",
    legend.justification = "center",
    legend.direction = "horizontal",
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  ) +
  scale_fill_manual(values = custom_colors) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_x_discrete(expand = expansion(add = 0.7)) +
  coord_fixed(ratio = 4 / 1) #changed from 3.2 to 4 for revision

ggsave(filename = paste0(fig_path, "Harmfulness_Score_per_Modality.svg"), plot = plot_2e, width = plot_width, height = plot_height, bg = "transparent")
```



```{r}
# Calculate attack success rate
attack_success_rate <- data_selection %>%
  group_by(Model, Modality, `Adversarial prompt`) %>%
  summarize(
    mean_LMR = mean(Harmfulness_Mean, na.rm = TRUE),
    sd_LMR = sd(Harmfulness_Mean, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  pivot_wider(
    names_from = `Adversarial prompt`,
    values_from = c(mean_LMR, sd_LMR),
    names_glue = "{.value}_{`Adversarial prompt`}"
  ) %>%
  mutate(
    Attack_Success_Rate = mean_LMR_1 - mean_LMR_0,
    SD_Attack_Success_Rate = sqrt(sd_LMR_1^2 + sd_LMR_0^2)  # Propagation of error
  )

# Ensure Modality is a factor with the correct order
attack_success_rate$Modality <- factor(attack_success_rate$Modality, levels = modality_order)

# Create the plot
plot_attack_success <- ggplot(attack_success_rate, aes(x = Modality, y = Attack_Success_Rate, fill = Model)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8)) +
  geom_errorbar(
    aes(ymin = pmax(Attack_Success_Rate - SD_Attack_Success_Rate, 0), 
        ymax = pmin(Attack_Success_Rate + SD_Attack_Success_Rate, 1)),
    width = 0, size = 0.5, color = "black", position = position_dodge(width = 0.8)
  ) +
  labs(y = "Attack Success Rate [%]") +
  theme_minimal() + 
  custom_theme + 
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = label_size, colour = "black", angle = 45, vjust = 1, hjust=1),
    axis.text.y = element_text(size = label_size, colour = "black"),
    axis.title.y = element_text(size = label_size, vjust = 2),
    legend.title = element_blank(),
    legend.text = element_text(size = label_size),
    legend.position = "top",
    legend.justification = "center",
    legend.direction = "horizontal",
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 1),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  ) +
  scale_fill_manual(values = custom_colors) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_x_discrete(expand = expansion(add = 0.7)) +
  coord_fixed(ratio = 4 / 1)

# Save the plot
ggsave(filename = paste0(fig_path, "Attack_Success_Rate_per_Modality.svg"), plot = plot_attack_success, width = plot_width, height = plot_height, bg = "transparent")

# Print summary statistics
print(attack_success_rate)
```





# # Suppl. Figure. 2 Circle Plot "PI Unveiled"
# 
```{r}
# 
# 
# # List of models to include
# models_to_include_2 <- c("GPT-4o", "Claude-3", "Claude-3.5", "Reka Core")
# 
# # Define label size
# label_size <- 22
# 
# # Loop through each model and create circle plots
# for (model in models_to_include_2) {
#   # Filter the data
#   filtered_data <- data %>%
#     filter(`Adversarial prompt` == "Prompt Injection") %>%
#     #filter(`Language of injection prompt` == "English") %>%
#     filter(Model == model) %>%
#     filter(!is.na(`Circle Plot`))
# 
#   # Summarize the counts
#   summary_counts <- filtered_data %>%
#     group_by(`Circle Plot`) %>%
#     summarize(count = n())
# 
#   # Create the circle plot
#   plot_circle <- ggplot(summary_counts, aes(x = "", y = count, fill = `Circle Plot`)) +
#     geom_col(width = 1, color = "white") +
#     geom_text(aes(label = count), 
#               position = position_stack(vjust = 0.5), 
#               color = "black", size = label_size * 0.6) +
#     coord_polar(theta = "y") +
#     labs(fill = "Prompt injection", x = NULL, y = NULL, title = model) +
#     theme_void() +
#     theme(
#       plot.title = element_text(size = label_size * 1.5, hjust = 0.5, vjust=-1),
#       legend.position = "right",
#       legend.direction = "vertical",
#       legend.title = element_text(size = label_size * 1.5),
#       legend.text = element_text(size = label_size * 1.5),
#       strip.text = element_text(size = label_size * 1.5),
#       plot.margin = margin(1, 1, 1, 1, "cm")
#     ) +
#     scale_fill_manual(values = c("successful" = "#fbc9c4", "failed" = "grey"))
# 
#   # Print the plot
#   print(plot_circle)
# 
#   # Save the plot
#   ggsave(filename = paste0(fig_path, "PI_Unveiled_Circle_Plot_", model, ".svg"), plot = plot_circle, width = 8, height = 6, bg = "transparent")
# }

```




# Summary statistics for Figure 2 (a)
```{r}
summary_figure_2a <- data %>%
  group_by(Model) %>%
  summarize(
    mean_accuracy = mean(Labels_Mean, na.rm = TRUE),
    sd_accuracy = sd(Labels_Mean, na.rm = TRUE),
    min_accuracy = min(Labels_Mean, na.rm = TRUE),
    max_accuracy = max(Labels_Mean, na.rm = TRUE)
  )

print(summary_figure_2a)


figure_2a_test <- kruskal.test(Labels_Mean ~ Model, data = data %>% filter(Model %in% c("Claude-3", "GPT-4o", "Claude-3.5", "Reka Core")))
print(figure_2a_test)

# Post-hoc analysis using Dunn test if Kruskal-Wallis test is significant

dunn_test <- dunnTest(Labels_Mean ~ Model, data = data %>% filter(Model %in% c("Claude-3", "GPT-4o", "Claude-3.5", "Reka Core")), method = "bonferroni")
print(dunn_test)


# Create a named list of p-values
p_values_list <- list(
  "Kruskal-Wallis Test" = figure_2a_test,
  "Dunn Test" = dunn_test$res
)

# Use the function
export_stats_to_excel(
  excel_file = suppl_path,
  sheet_name = "ST4 Statistics Figure 2a",
  summary_stats = summary_figure_2a,
  p_values = p_values_list
)
```


```{r}
summary_stats_2a <- data_long %>%
  group_by(Model) %>%
  summarize(
    mean_labels = mean(Labels, na.rm = TRUE),
    sd_labels = sd(Labels, na.rm = TRUE),
    .groups = 'drop'
  )
```


# Summary statistics for Figure 2 (b)
```{r}






# Summary statistics for Figure 2b
summary_figure_2b <- data_selection %>%
  group_by(Model, `Adversarial prompt`) %>%
  summarize(
    mean_harmfulness = mean(Harmfulness_Mean, na.rm = TRUE),
    sd_harmfulness = sd(Harmfulness_Mean, na.rm = TRUE),
    min_harmfulness = min(Harmfulness_Mean, na.rm = TRUE),
    max_harmfulness = max(Harmfulness_Mean, na.rm = TRUE)
  )

print(summary_figure_2b)

attack_success_rate <- summary_figure_2b %>%
  select(Model, `Adversarial prompt`, mean_harmfulness) %>%
  pivot_wider(names_from = `Adversarial prompt`, values_from = mean_harmfulness) %>%
  mutate(Attack_Success_Rate = `Prompt Injection` - `No Prompt Injection`) %>%
  select(Model, Attack_Success_Rate)

print(attack_success_rate)


# Initialize a list to store p-values
p_values_list_within <- list()
p_values_list_between <- list()

# Wilcoxon Signed-Rank Test for Prompt Injection vs No Prompt Injection within each model
for (model in unique(data_selection$Model)) {
  test_result <- wilcox.test(Harmfulness_Mean ~ `Adversarial prompt`, data = data_selection %>% filter(Model == model))
  p_values_list_within[[paste0("Prompt vs No Prompt - ", model)]] <- test_result$p.value
}

# # Pairwise Mann-Whitney U Tests for Harmfulness Scores between Models for each Adversarial Prompt condition
# models_to_compare <- unique(data_selection$Model)
# adversarial_conditions <- unique(data_selection$`Adversarial prompt`)
# 
# for (condition in adversarial_conditions) {
#   for (i in 1:(length(models_to_compare) - 1)) {
#     for (j in (i + 1):length(models_to_compare)) {
#       model_pair <- paste0(models_to_compare[i], " vs ", models_to_compare[j])
#       test_result <- wilcox.test(Harmfulness_Mean ~ Model, data = data_selection %>% filter(Model %in% c(models_to_compare[i], models_to_compare[j]), `Adversarial prompt` == condition))
#       p_values_list_between[[paste0(model_pair, " - ", condition)]] <- test_result$p.value
#     }
#   }
# }

# Combine p-values into vectors
all_p_values_within <- unlist(p_values_list_within)
# all_p_values_between <- unlist(p_values_list_between)

# Adjust p-values for multiple testing (Bonferroni)
adjusted_p_values_within <- p.adjust(all_p_values_within, method = "bonferroni")
# adjusted_p_values_between <- p.adjust(all_p_values_between, method = "bonferroni")

# Create data frames for the results
results_within_df <- data.frame(
  Test = names(p_values_list_within),
  P_Value = all_p_values_within,
  Adjusted_P_Value = adjusted_p_values_within
)

# results_between_df <- data.frame(
#   Test = names(p_values_list_between),
#   P_Value = all_p_values_between,
#   Adjusted_P_Value = adjusted_p_values_between
# )

# Print the results
print("Adjusted p-values for Figure 2b (Within Models):")
print(results_within_df)

# print("Adjusted p-values for Figure 2b (Between Models):")
# print(results_between_df)

mean_harmfulness_overall <- data_selection %>%
  group_by(`Adversarial prompt`) %>%
  summarize(
    mean_harmfulness = mean(Harmfulness_Mean, na.rm = TRUE),
    sd_harmfulness = sd(Harmfulness_Mean, na.rm = TRUE),
    min_harmfulness = min(Harmfulness_Mean, na.rm = TRUE),
    max_harmfulness = max(Harmfulness_Mean, na.rm = TRUE)
  )

print("Mean Harmfulness Over All Models:")
print(mean_harmfulness_overall)


data_no_prompt <- data_selection %>%
  filter(`Adversarial prompt` == "No Prompt Injection") %>%
  pull(Harmfulness_Mean)

data_prompt <- data_selection %>%
  filter(`Adversarial prompt` == "Prompt Injection") %>%
  pull(Harmfulness_Mean)

# Mann-Whitney U Test
mann_whitney_test <- wilcox.test(data_no_prompt, data_prompt, paired = FALSE)

# Print the results
print("Mann-Whitney U Test for mean harmfulness over all models:")
print(mann_whitney_test)


# Organize the p-values and test results
p_values_list <- list(
  "Mean Lesion Miss Rate Over All Models" = mean_harmfulness_overall,
  "Attack Success Rate" = attack_success_rate,
  "Within Models (Prompt vs No Prompt), Two-Sided Wilcoxon-Signed Rank test + Bonferroni" = results_within_df,
  # "Between Models" = results_between_df,
  "Two sided Mann-Whitney U Test (Over all models combined for prompt injection/no prompt injection:)" = mann_whitney_test
  
)

# Apply the function
export_stats_to_excel(
  excel_file = suppl_path,
  sheet_name = "ST5 Statistics Figure 2b",
  summary_stats = summary_figure_2b,
  p_values = p_values_list
)

```
```{r}
print(summary_figure_2b)
print(attack_success_rate)
```



# Summary statistics for Figure 2 (c)
```{r}

summary_figure_2c <- data_selection %>%
  group_by(Model, `Position of adversarial prompt`) %>%
  summarize(
    mean_harmfulness = mean(Harmfulness_Mean, na.rm = TRUE),
    sd_harmfulness = sd(Harmfulness_Mean, na.rm = TRUE),
    min_harmfulness = min(Harmfulness_Mean, na.rm = TRUE),
    max_harmfulness = max(Harmfulness_Mean, na.rm = TRUE)
  )

print(summary_figure_2c)

# Compare significance of Prompt injection over all models together
p_values_list_position <- list()
positions_to_compare <- unique(data_selection$`Position of adversarial prompt`)

for (position in positions_to_compare) {
  if (position != "No Prompt Injection") {
    data_no_prompt <- data_selection %>%
      filter(`Position of adversarial prompt` == "No Prompt Injection") %>%
      pull(Harmfulness_Mean)
    
    data_position <- data_selection %>%
      filter(`Position of adversarial prompt` == position) %>%
      pull(Harmfulness_Mean)
    
    test_result <- wilcox.test(data_no_prompt, data_position, paired = FALSE)
    p_values_list_position[[paste0(position, " vs No Prompt Injection")]] <- test_result$p.value
  }
}

# Combine p-values into a vector
all_p_values_position <- unlist(p_values_list_position)

# Adjust p-values for multiple testing (Bonferroni)
adjusted_p_values_position <- p.adjust(all_p_values_position, method = "bonferroni")

# Create a data frame for the results
results_position_df <- data.frame(
  Test = names(p_values_list_position),
  P_Value = all_p_values_position,
  Adjusted_P_Value = adjusted_p_values_position
)

# Print the results
print("Adjusted p-values for Figure 2c (Position of Adversarial Prompt):")
print(results_position_df)



# Organize the p-values and test results
p_values_list <- list(
  "Position of Adversarial Prompt Comparisons (Two-sided Mann-Whitney U Test with bonferroni correction)" = results_position_df
)

# Apply the function
export_stats_to_excel(
  excel_file = suppl_path,
  sheet_name = "ST6 Statistics Figure 2c",
  summary_stats = summary_figure_2c,
  p_values = p_values_list
)





```


# Summary statistics for Figure 2 (d)
```{r}

summary_figure_2d <- data_selection %>%
  group_by(Model, Variation) %>%
  summarize(
    mean_harmfulness = mean(Harmfulness_Mean, na.rm = TRUE),
    sd_harmfulness = sd(Harmfulness_Mean, na.rm = TRUE),
    min_harmfulness = min(Harmfulness_Mean, na.rm = TRUE),
    max_harmfulness = max(Harmfulness_Mean, na.rm = TRUE)
  )

print(summary_figure_2d)



p_values_list_variation <- list()

# Mann-Whitney U Test for each "Variation" compared to "No Prompt Injection"
variations_to_compare <- unique(data_selection$Variation)

for (variation in variations_to_compare) {
  if (variation != "No Prompt Injection") {
    data_no_prompt <- data_selection %>%
      filter(Variation == "No Prompt Injection") %>%
      pull(Harmfulness_Mean)
    
    data_variation <- data_selection %>%
      filter(Variation == variation) %>%
      pull(Harmfulness_Mean)
    
    test_result <- wilcox.test(data_no_prompt, data_variation, paired = FALSE)
    p_values_list_variation[[paste0(variation, " vs No Prompt Injection")]] <- test_result$p.value
  }
}

# Combine p-values into a vector
all_p_values_variation <- unlist(p_values_list_variation)

# Adjust p-values for multiple testing (Bonferroni)
adjusted_p_values_variation <- p.adjust(all_p_values_variation, method = "bonferroni")

# Create a data frame for the results
results_variation_df <- data.frame(
  Test = names(p_values_list_variation),
  P_Value = all_p_values_variation,
  Adjusted_P_Value = adjusted_p_values_variation
)

# Print the results
print("Adjusted p-values for Variation (compared to No Prompt Injection):")
print(results_variation_df)



# Organize the p-values and test results
p_values_list <- list(
  "Variation Comparisons (vs No Prompt Injection) (Two-sided Mann-Whitney U Test with bonferroni correction)" = results_variation_df
)

# Apply the function
export_stats_to_excel(
  excel_file = suppl_path,
  sheet_name = "ST7 Statistics Figure 2d",
  summary_stats = summary_figure_2d,
  p_values = p_values_list
)

```


# Stat Suppl. Fig 3a
```{r}
modalities_to_compare <- unique(data_selection$Modality)
p_values_list_modality <- list()

# Perform pairwise comparisons
for (i in 1:(length(modalities_to_compare) - 1)) {
  for (j in (i + 1):length(modalities_to_compare)) {
    modality1 <- modalities_to_compare[i]
    modality2 <- modalities_to_compare[j]
    
    data_modality1 <- data_selection %>%
      filter(Modality == modality1) %>%
      pull(Labels_Mean)
    
    data_modality2 <- data_selection %>%
      filter(Modality == modality2) %>%
      pull(Labels_Mean)
    
    test_result <- wilcox.test(data_modality1, data_modality2, paired = FALSE)
    p_values_list_modality[[paste0(modality1, " vs ", modality2)]] <- test_result$p.value
  }
}

# Combine p-values into a vector
all_p_values_modality <- unlist(p_values_list_modality)

# Adjust p-values for multiple testing (Bonferroni)
adjusted_p_values_modality <- p.adjust(all_p_values_modality, method = "bonferroni")

# Create a data frame for the results
results_modality_df <- data.frame(
  Test = names(p_values_list_modality),
  P_Value = all_p_values_modality,
  Adjusted_P_Value = adjusted_p_values_modality
)

# Print the results
print("Adjusted p-values for Organ detection Modality comparisons,: (Two-sided Mann-Whitney U Test with bonferroni correction)")
print(results_modality_df)

# Prepare the data for export
p_values_for_export <- list(
  "Organ detection rate per Modality (Two-sided Mann-Whitney U Test with bonferroni correction)" = results_modality_df
)


export_stats_to_excel(
  excel_file = suppl_path,
  sheet_name = "ST8 Statistics Figure 3a",
  summary_stats = summary_stats_2a2,
  p_values = p_values_for_export  # Empty list for p_values
)


```





# Stat Fig 3b
```{r}
# Original summary
summary_figure_2e <- data_selection %>%
  group_by(Model, Modality) %>%
  summarize(
    mean_harmfulness = mean(Harmfulness_Mean, na.rm = TRUE),
    sd_harmfulness = sd(Harmfulness_Mean, na.rm = TRUE),
    min_harmfulness = min(Harmfulness_Mean, na.rm = TRUE),
    max_harmfulness = max(Harmfulness_Mean, na.rm = TRUE),
    .groups = 'drop'
  )



################################### Statistical Tests ##################################

modalities_to_compare <- unique(data_selection$Modality)
p_values_list_modality <- list()

# Perform pairwise comparisons
for (i in 1:(length(modalities_to_compare) - 1)) {
  for (j in (i + 1):length(modalities_to_compare)) {
    modality1 <- modalities_to_compare[i]
    modality2 <- modalities_to_compare[j]
    
    data_modality1 <- data_selection %>%
      filter(Modality == modality1) %>%
      pull(Harmfulness_Mean)
    
    data_modality2 <- data_selection %>%
      filter(Modality == modality2) %>%
      pull(Harmfulness_Mean)
    
    test_result <- wilcox.test(data_modality1, data_modality2, paired = FALSE)
    p_values_list_modality[[paste0(modality1, " vs ", modality2)]] <- test_result$p.value
  }
}

all_p_values_modality <- unlist(p_values_list_modality) # Combine p-values into a vector
adjusted_p_values_modality <- p.adjust(all_p_values_modality, method = "bonferroni") 

# Create a data frame for the results
results_modality_df <- data.frame(
  Test = names(p_values_list_modality),
  P_Value = all_p_values_modality,
  Adjusted_P_Value = adjusted_p_values_modality
)

# Print the results
print("Adjusted p-values for Modality comparisons:")
print(results_modality_df)


# Calculate harmfulness separately for No Prompt Injection and Prompt Injection
harmfulness_by_prompt <- data_long %>%
  group_by(Model, Modality, `Adversarial prompt`) %>%
  summarize(
    mean_harmfulness = mean(Harmfulness, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  pivot_wider(
    names_from = `Adversarial prompt`, 
    values_from = mean_harmfulness,
    names_prefix = "harmfulness_"
  )

print(harmfulness_by_prompt)

attack_success_rate <- harmfulness_by_prompt %>%
  mutate(
    Attack_Success_Rate = `harmfulness_Prompt Injection` - `harmfulness_No Prompt Injection`) #%>%
  #select(Model, Modality, Attack_Success_Rate)

# Summarize Attack Success Rate per modality across all models
attack_success_rate_per_modality <- attack_success_rate %>%
  group_by(Modality) %>%
  summarize(
    mean_attack_success_rate = mean(Attack_Success_Rate, na.rm = TRUE),
    sd_attack_success_rate = sd(Attack_Success_Rate, na.rm = TRUE),
    min_attack_success_rate = min(Attack_Success_Rate, na.rm = TRUE),
    max_attack_success_rate = max(Attack_Success_Rate, na.rm = TRUE),
    .groups = 'drop'
  )

attack_success_rate_per_model <- attack_success_rate %>%
  group_by(Model) %>%
  summarize(
    mean_attack_success_rate = mean(Attack_Success_Rate, na.rm = TRUE),
    sd_attack_success_rate = sd(Attack_Success_Rate, na.rm = TRUE),
    min_attack_success_rate = min(Attack_Success_Rate, na.rm = TRUE),
    max_attack_success_rate = max(Attack_Success_Rate, na.rm = TRUE),
    .groups = 'drop'
  )



attack_success_rate <- attack_success_rate %>%
  left_join(attack_success_rate_per_modality %>% select(Modality, sd_attack_success_rate), by = "Modality")


attack_success_rate$Attack_Success_Rate <- as.numeric(attack_success_rate$Attack_Success_Rate)

attack_success_rate <- attack_success_rate %>%
  mutate(Attack_Success_Rate = ifelse(Attack_Success_Rate < 0, 0, Attack_Success_Rate))

# Combine original summary with Attack Success Rate
summary_with_attack_rate <- summary_figure_2e %>%
  left_join(attack_success_rate, by = c("Model", "Modality"))

print("Summary with Attack Success Rate:")
print(summary_with_attack_rate)

print("Attack Success Rate per Modality (across all models):")
print(attack_success_rate_per_modality)

# Unified export
final_summary_stats <- summary_stats_combined %>%
  left_join(attack_success_rate, by = c("Model", "Modality"))

final_p_values_list <- list(
  "Lesion Miss Rate per Modality (Two-sided Mann-Whitney U Test with bonferroni correction)" = results_modality_df,
  "Attack Success Rate" = attack_success_rate,
  "Attack Success Rate per Modality" = attack_success_rate_per_modality,
  "Attack Success Rate per Model" = attack_success_rate_per_model
)

# Apply the function to export to Excel
export_stats_to_excel(
  excel_file = suppl_path,
  sheet_name = "ST9 Statistics Figure 3b",
  summary_stats = final_summary_stats,
  p_values = final_p_values_list
)
```
